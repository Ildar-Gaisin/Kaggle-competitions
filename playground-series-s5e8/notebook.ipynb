{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16b648-4774-4b42-81a3-ac260c4d0ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path1 = \"/home/ildar/.cache/kagglehub/datasets/amruthayenikonda/dirty-dataset-to-practice-data-cleaning/versions/1/train.csv\"\n",
    "\n",
    "path2 = \"/home/ildar/.cache/kagglehub/datasets/sushant097/bank-marketing-dataset-full/versions/1/bank-full.csv\"\n",
    "\n",
    "train_data = pd.read_csv(path1, index_col = \"id\")\n",
    "extra_data = pd.read_csv(path2, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65cbd9-844e-4fe6-916c-40394bb87cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d9f89e-3daa-4303-864d-3133187679f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcead567-53aa-4bd3-bb7e-f04b4f61b4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea72cb4-0571-43d3-9ad7-dad1ebbb131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"y\"\n",
    "NUMS = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "CATS = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ec86b3-0abe-4421-bd43-8acb103129af",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_data[\"y\"] = extra_data[\"y\"].apply(lambda x:1 if x==\"yes\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a5a1b-57ae-4b1c-8854-5ad31d3d6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([train_data, extra_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919cef0-9beb-4f21-a444-32061be7963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(3,3, figsize=(12,8))\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, col in enumerate(CATS):\n",
    "    train_data[col].value_counts().plot(kind=\"bar\", ax = axs[i])\n",
    "    axs[i].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc66a3d-86a2-411c-8ac9-b3ba99c1ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.hist(bins=50, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef343b0-56e9-4ba9-b4a2-2e99bce3b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(df_train[NUMS].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdda0200-e0ff-4b0e-b3df-45dcf2cc82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"duration\"].hist(bins=50, figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6fb904-c8b6-4bd5-8b5e-54716af07f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.plot(kind=\"scatter\", x=\"pdays\", y=\"day\", figsize=(12,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d2faa-1dcd-4585-aa99-7f239fa03f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyclical features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df_train['_pdays_sin'] = np.sin(2*np.pi * df_train['pdays'] / 365).astype('float32')\n",
    "df_train['_pdays_cos'] = np.cos(2*np.pi*df_train['pdays']/365).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47d1a08-542f-483f-9006-74689686058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert balance and duration to float32\n",
    "\n",
    "df_train[\"balance\"] = df_train[\"balance\"].astype(\"float32\")\n",
    "df_train[\"duration\"] = df_train[\"duration\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4cacb3-6cfa-44a7-9a45-dc02a34fe47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert day and month attribute to date which is one number in a year\n",
    "\n",
    "\n",
    "replacement_values = {'may': 5,\n",
    " 'aug': 8,\n",
    " 'jul': 7,\n",
    " 'jun': 6,\n",
    " 'nov': 11,\n",
    " 'apr': 4,\n",
    " 'feb': 2,\n",
    " 'jan': 1,\n",
    " 'oct': 10,\n",
    " 'sep': 9,\n",
    " 'mar': 3,\n",
    " 'dec': 12}\n",
    "\n",
    "df_train[\"data\"] = (df_train[\"month\"].replace(replacement_values)-1)*12+df_train[\"day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1c3d35-1c2a-4a2b-b270-5d8404c0f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"_date_cos\"] = np.cos(2*np.pi*df_train[\"data\"]/365).astype(\"float32\")\n",
    "df_train[\"_date_sin\"] = np.sin(2*np.pi*df_train[\"data\"]/365).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15228a6-8359-4eff-8def-cfb3e91369cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"was_not_contacted\"] = (df_train[\"pdays\"]==-1).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e72def9-4912-4a97-8772-8e0ad66a21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert duration and balance by logarithms\n",
    "\n",
    "df_train[\"balance_log\"] = np.sign(df_train[\"balance\"])*np.log1p(np.abs(df_train[\"balance\"]))\n",
    "df_train[\"duration_log\"] = np.log1p(df_train[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb524957-798c-47e4-a055-1141741716e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_train[\"y\"]\n",
    "df_train.drop(\"y\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa1c323-ee0e-414e-84dd-650c108b9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert int64 and object dtypes to category\n",
    "cols = df_train.select_dtypes([\"int64\", \"object\"]).columns.to_list()\n",
    "\n",
    "for col in cols:\n",
    "    df_train[col] = df_train[col].astype(\"category\")\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e084af12-d7b9-4718-bd63-4c3799871e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally let's do interactions\n",
    "from itertools import combinations\n",
    "\n",
    "CATS = df_train.select_dtypes(\"category\").columns.to_list()\n",
    "NUMS = df_train.select_dtypes(\"float32\").columns.to_list()\n",
    "\n",
    "for col in combinations(CATS,2):\n",
    "    name = \"_\".join(col)\n",
    "    df_train[name] = (df_train[col[0]].astype(\"str\")+\"_\"+df_train[col[1]].astype(\"str\")).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc88a6c7-9b2f-467a-ae29-b7868a24d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combinations(NUMS, 2):\n",
    "    name = \"x\".join(col)\n",
    "    df_train[name] = df_train[col[0]]*df_train[col[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb243fe9-74cf-4cd1-8cc6-deef250681e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0d3ac-841e-435e-826b-cce860d350d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = df_train.select_dtypes(\"category\").columns.to_list()\n",
    "NUMS = df_train.select_dtypes(\"number\").columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361d62f-0953-441a-b7a1-66cbfb43a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77795cd1-60aa-4750-836f-5d01604e289f",
   "metadata": {},
   "source": [
    "## 1) Hyperparameter tuning for TabM_D_Classifier from PyTabkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a6da6-e4ac-442c-ab27-f60fe61b5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#approach using optuna\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "from pytabkit import TabM_D_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    smoothing = trial.suggest_float(\"smoothing\", 1e-5, 1e0, log=True)\n",
    "\n",
    "\n",
    "    param_TabM_inputs = {\n",
    "        'device': 'cuda',\n",
    "        'val_metric_name': '1-auc_ovr',\n",
    "        'random_state': 100,\n",
    "        'verbosity': 2,\n",
    "        'arch_type': 'tabm-mini',\n",
    "        'tabm_k': 32,\n",
    "        'num_emb_type': 'pwl',\n",
    "        'd_embedding': 12,\n",
    "        'batch_size': 256,\n",
    "        'lr': lr,\n",
    "        'n_epochs': 10,\n",
    "        'dropout': 0.1,\n",
    "        'd_block': 512,\n",
    "        'n_blocks': 3\n",
    "    }\n",
    "    \n",
    "\n",
    "    #define TargetEncoder and StandardScaler    \n",
    "\n",
    "\n",
    "    #model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    fold_ROC = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "        df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "        y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "        #instantiate and fit column transformer\n",
    "        col_tran = ColumnTransformer([(\"cat\", TargetEncoder(cols=CATS, smoothing=smoothing), CATS), \n",
    "                                  (\"num\", StandardScaler(), NUMS)])#, remainder=\"passthrough\")\n",
    "        df_train_cross = col_tran.fit_transform(df_train_cross, y_train)\n",
    "        df_val_cross = col_tran.transform(df_val_cross)\n",
    "\n",
    "        #instantiate and fit tabM model\n",
    "        tab_m = TabM_D_Classifier(**param_TabM_inputs)\n",
    "\n",
    "        tab_m.fit(df_train_cross, y_train)\n",
    "        oof = tab_m.predict_proba(df_val_cross)[:,1]\n",
    "        error = roc_auc_score(y_val, oof)\n",
    "        fold_ROC.append(error)\n",
    "\n",
    "        del tab_m\n",
    "        del df_train_cross, df_val_cross, y_train, y_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return sum(fold_ROC)/len(fold_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d303a-25e3-4132-b970-409025db3f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1, n_jobs=1, show_progress_bar=True)\n",
    "torch.cuda.empty_cache()  #clear reserved memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568fbb54-ba90-4fa5-80ea-045a6d38dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7dd9b-4ae3-4ef9-bd61-d7b65834458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323b54f-e0e1-4932-a81c-162f58ae7e83",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter tuning with xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd4881-2501-40f9-8e34-38ff8ec4ca12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    smoothing = trial.suggest_float(\"smoothing\", 1e-5, 1e0, log=True)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 8)\n",
    "    n_estimators=trial.suggest_int(\"n_est\", 500, 1000)\n",
    "    \n",
    "    #model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    fold_ROC = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "        df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "        y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "        preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),\n",
    "                            (\"nums\", StandardScaler(), NUMS)])\n",
    "        est = XGBClassifier(device = \"cuda\", learning_rate=lr, max_depth=max_depth, n_estimators=n_estimators)\n",
    "        model = make_pipeline(preproc, est)\n",
    "\n",
    "        model.fit(df_train_cross, y_train)\n",
    "        oof = model.predict_proba(df_val_cross)[:,1]\n",
    "        error = roc_auc_score(y_val, oof)\n",
    "        fold_ROC.append(error)\n",
    "\n",
    "        del model\n",
    "        del df_train_cross, df_val_cross, y_train, y_val\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "    return sum(fold_ROC)/len(fold_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50b4a0-f3b6-425e-80a8-47c4293de8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_forest = optuna.create_study(direction=\"maximize\")\n",
    "study_forest.optimize(objective, n_trials=10, n_jobs=1, show_progress_bar=True)\n",
    "torch.cuda.empty_cache()  #clear reserved memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52839b18-6167-4ede-847b-40eab6d8680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_forest.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a280151-7551-4903-b784-35f6459bbc0e",
   "metadata": {},
   "source": [
    "## 3) Hyperparameter tuning with custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8515f2-0c41-4e10-b3ae-f568154c631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_data, cat_cols, num_cols, train_target):\n",
    "\n",
    "        m = OrdinalEncoder(dtype=np.int64)\n",
    "\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy[cat_cols] = m.fit_transform(train_data_copy[cat_cols])\n",
    "\n",
    "        self.train_cat = train_data_copy[cat_cols].values\n",
    "        self.train_num = train_data_copy[num_cols].values\n",
    "\n",
    "        self.target = train_target.values\n",
    "        \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_cat[idx], self.train_num[idx], self.target[idx]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_cat)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    cat_features = [item[0] for item in batch]\n",
    "    num_features = [item[1] for item in batch]\n",
    "    targets = [item[2] for item in batch]\n",
    "\n",
    "    cat_features = torch.tensor(cat_features, dtype=torch.int64)\n",
    "    num_features = torch.tensor(num_features, dtype=torch.float32)\n",
    "    targets = torch.tensor(targets, dtype=torch.int64)\n",
    "    \n",
    "    return [cat_features, num_features, targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba34ce2-2280-4b24-a52c-eeae6015c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = MyDataset(df_train, CATS, NUMS, df_target)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, collate_fn=collate_fn)\n",
    "\n",
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1330e4fe-0f04-4bb2-9e2a-aba6a6a07683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "#custom model\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(self, cat_cols, num_cols, num_hiddens, ffn_num_hiddens, num_heads, num_blks, dropout):\n",
    "        super().__init__()\n",
    "        self.num_hiddens=num_hiddens\n",
    "        self.cat_cols = cat_cols\n",
    "        \n",
    "        #create a (learnable) embedding vector for each categorical column\n",
    "        for col in cat_cols:\n",
    "            name = \"token_embedding_\" + col\n",
    "            vocab_size = len(df_train[col].value_counts())\n",
    "            setattr(self, name, nn.Embedding(vocab_size, num_hiddens))\n",
    "        \n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_blks):\n",
    "            self.blks.add_module(f\"{i}\", nn.TransformerEncoderLayer(d_model=num_hiddens, \n",
    "                                                                   nhead=num_heads,\n",
    "                                                                   dim_feedforward=ffn_num_hiddens,\n",
    "                                                                   dropout=dropout, \n",
    "                                                                   batch_first=True))\n",
    "\n",
    "        self.cont_layer_norm = nn.LayerNorm(len(num_cols))\n",
    "        \n",
    "        self.mlp = nn.Sequential(nn.LazyLinear(num_hiddens), nn.ReLU(), nn.LayerNorm(num_hiddens), nn.LazyLinear(2))\n",
    "\n",
    "    def forward(self, cat_input_ids, num_input):\n",
    "        tensor_list = []\n",
    "        for i, col in enumerate(self.cat_cols):\n",
    "            name = \"token_embedding_\" + col\n",
    "            embed = getattr(self, name)\n",
    "            tensor_list.append(embed(cat_input_ids[:,i]))  #this will be ith column\n",
    "\n",
    "        X = torch.stack(tensor_list, dim=0)\n",
    "        X = X.permute(1,0,2)\n",
    "\n",
    "        for blk in self.blks:\n",
    "            X  = blk(X)\n",
    "\n",
    "        X = X.permute(1,0,2)\n",
    "\n",
    "        X = torch.cat(list(X), dim=1)  #output should be batch_size*(num_cat_features*hidden_dim)\n",
    "\n",
    "        Y = self.cont_layer_norm(num_input)    #output will be batch_size*(num_num_features)\n",
    "\n",
    "        X = torch.cat([X, Y], dim=1)  #output will be 2-dim matrix of batch_size*TOTAL_hidden_dim\n",
    "\n",
    "        X = self.mlp(X)\n",
    "\n",
    "        \n",
    "            \n",
    "        return X\n",
    "\n",
    "    \n",
    "    def apply_init(self, inputs, init=None):\n",
    "        self.forward(*inputs)\n",
    "        if init is not None:\n",
    "            self.mlp.apply(init)\n",
    "            self.blks.apply(init)\n",
    "\n",
    "    def init_weights_xavier_uniform(self, m):\n",
    "        if hasattr(m, 'weight') and m.weight.dim()>=2:\n",
    "            nn.init.xavier_uniform_(m.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae88d0-c8c1-4a74-bd3d-4243c528fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "X = torch.tensor([[-3,2],[2.1,1],[-10,2], [2,3],[2.122,0.283892],[2.1234123,0.13412]])\n",
    "softmax_layer = nn.Softmax(dim=1)\n",
    "X = softmax_layer(X)\n",
    "X = X[:,1]\n",
    "\n",
    "binary_auroc(X, torch.tensor([1,0,1,1,1,0])).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985b7ea-701d-4b27-8955-70495908c9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "from torcheval.metrics.functional import binary_auroc\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    \n",
    "    weight_decay = trial.suggest_float(\"Ridge\", 1e-5, 1e0, log=True)\n",
    "\n",
    "    #define model, optimizer and criterion    \n",
    "    model = TabTransformer(cat_cols=CATS, num_cols=NUMS, num_hiddens=32, ffn_num_hiddens=128, num_heads=8, num_blks=6, dropout=0.2)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    #define data and cross-validation splitting values\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    fold_accuracies=[]\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "\n",
    "        df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "        y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "        train_subset = MyDataset(df_train_cross, CATS, NUMS, y_train)\n",
    "        val_subset = MyDataset(df_val_cross, CATS, NUMS, y_val)\n",
    "        \n",
    "\n",
    "        train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=1024, shuffle=True, pin_memory=True, num_workers=10, \n",
    "                                                       collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "        val_dataloader = torch.utils.data.DataLoader(val_subset, batch_size=1024, shuffle=False, pin_memory=True, num_workers=10, \n",
    "                                                    collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "        \n",
    "        #re-initialize weights here\n",
    "        init_inputs = (next(iter(train_dataloader)))[:2]\n",
    "        for i, v in enumerate(init_inputs):\n",
    "            v = v.to(device, non_blocking=True)\n",
    "            init_inputs[i]=v\n",
    "            \n",
    "        model.apply_init(init_inputs, model.init_weights_xavier_uniform)\n",
    "\n",
    "        num_epochs=5\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for inputs in train_dataloader:\n",
    "                #move data to GPU\n",
    "                for k,v in enumerate(inputs):\n",
    "                    v = v.to(device, non_blocking=True)\n",
    "                    inputs[k] = v\n",
    "                \n",
    "                outputs = model(*inputs[:2])\n",
    "                loss = criterion(outputs, inputs[2])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                del inputs\n",
    "                del loss\n",
    "                del outputs\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs in val_dataloader:\n",
    "\n",
    "                for k, v in enumerate(inputs):\n",
    "                    v = v.to(device, non_blocking=True)\n",
    "                    inputs[k] = v\n",
    "\n",
    "                outputs = model(*inputs[:2]) #mlm_Y_hat and nsp_Y_hat\n",
    "\n",
    "                softmax_layer = nn.Softmax(dim=1)\n",
    "                outputs = softmax_layer(outputs)[:,1]\n",
    "                roc_score = binary_auroc(outputs, inputs[2]).item()\n",
    "                \n",
    "\n",
    "                del inputs\n",
    "                del outputs\n",
    "\n",
    "        fold_accuracies.append(roc_score)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return sum(fold_accuracies)/len(fold_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51aea6a-e4a6-4ed3-acec-2b08face747f",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_tabtrans = optuna.create_study(direction=\"maximize\")\n",
    "study_tabtrans.optimize(objective, n_trials=5, n_jobs=1, show_progress_bar=True)\n",
    "torch.cuda.empty_cache()  #clear reserved memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b77f7f-376c-485b-970f-919efc49b3f6",
   "metadata": {},
   "source": [
    "## 4) Hyperparameter Tuning with LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53de48f-a1d9-42c8-8b52-1da808a8dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    smoothing = trial.suggest_float(\"smoothing\", 1e-5, 1e0, log=True)\n",
    "    n_estimators=trial.suggest_int(\"n_est\", 500, 1000)\n",
    "    num_leaves = trial.suggest_int(\"n_leaves\",10,30)\n",
    "\n",
    "    params = {\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": {\"l2\", \"l1\"},\n",
    "    \"num_leaves\": num_leaves,\n",
    "    \"learning_rate\": lr,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"verbose\": -1,\n",
    "    \"device_type\":\"cuda\",\n",
    "    \"n_estimators\":n_estimators\n",
    "    }\n",
    "    \n",
    "    #model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    fold_ROC = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "        df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "        y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "        preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),(\"nums\", StandardScaler(), NUMS)])\n",
    "\n",
    "        df_train_cross = preproc.fit_transform(df_train_cross, y_train)\n",
    "        df_val_cross = preproc.transform(df_val_cross)\n",
    "\n",
    "    \n",
    "        lgb_train = lgb.Dataset(df_train_cross, y_train, categorical_feature=None)\n",
    "        \n",
    "        lgb_eval = lgb.Dataset(df_val_cross, y_val, reference=lgb_train, categorical_feature=None)\n",
    "\n",
    "        #gbm = lgb.LGBMClassifier(**params)\n",
    "\n",
    "        gbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "    \n",
    "        #gbm.fit(df_train_cross, y_train)\n",
    "        oof = gbm.predict(df_val_cross, type=\"response\")\n",
    "        error = roc_auc_score(y_val, oof)\n",
    "        fold_ROC.append(error)\n",
    "\n",
    "        del gbm\n",
    "        del df_train_cross, df_val_cross, y_train, y_val\n",
    "        gc.collect()\n",
    "        \n",
    "    return sum(fold_ROC)/len(fold_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1ee94-b001-413a-b0d4-15a40be19187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "study_lgb.optimize(objective, n_trials=10, n_jobs=1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4569c02-a491-4514-bd2c-03918101c137",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_lgb.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09373dc4-952f-4c4a-b2cb-dd91a117b0b4",
   "metadata": {},
   "source": [
    "## 5) Hyperparameter Tuning with RealMLP model from PyTabkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b474af2b-aae6-49d1-86b9-66ca2d308b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytabkit import RealMLP_TD_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")    #requires tensor cores. Trades precision for performance\n",
    "\n",
    "df_train_realmlp = df_train.copy()\n",
    "\n",
    "for col in CATS:\n",
    "    df_train_realmlp[col] = df_train_realmlp[col].cat.codes\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e0, log=True)\n",
    "    \n",
    "    #model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "    fold_ROC = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "        df_train_cross, df_val_cross = df_train_realmlp.loc[train_idx, CATS+NUMS], df_train_realmlp.loc[val_idx, CATS+NUMS]\n",
    "        y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "        model = RealMLP_TD_Classifier(device='cuda:0', random_state=0, n_cv=1, n_refit=0,\n",
    "                                  n_epochs=10, batch_size=1024, hidden_sizes=[256] * 3,\n",
    "                                  val_metric_name='cross_entropy',\n",
    "                                  use_ls=False,\n",
    "                                  lr=lr, \n",
    "                                  verbosity=-1, val_fraction=0)\n",
    "        \n",
    "        model.fit(df_train_cross, y_train, cat_col_names=CATS)\n",
    "        oof = model.predict_proba(df_val_cross)[:,1]\n",
    "\n",
    "        error = roc_auc_score(y_val, oof)\n",
    "        fold_ROC.append(error)\n",
    "\n",
    "        del model, oof\n",
    "        del df_train_cross, df_val_cross, y_train, y_val\n",
    "        gc.collect()\n",
    "        \n",
    "    return sum(fold_ROC)/len(fold_ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7e2a0-e868-412f-8fda-e5d843137a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "study_realmlp = optuna.create_study(direction=\"maximize\")\n",
    "study_realmlp.optimize(objective, n_trials=10, n_jobs=1, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d12056-d667-44a2-920d-e9af359e37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_realmlp.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e404c08-1a72-4293-b95e-61b2144ad15a",
   "metadata": {},
   "source": [
    "## Construct OOF data for metamodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f226cc-9ef0-4d60-b332-65d4ac56c304",
   "metadata": {},
   "source": [
    "### 0) Set-up input for the meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c562ed-73f6-41ad-8b86-6817c16cbdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_tensor_for_meta_model = torch.zeros(len(df_train), 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992830c-2621-450c-9be0-b346ee6edb00",
   "metadata": {},
   "source": [
    "### 1) OOF_predictions with best found hyperparameters for TabM_D_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d16fdf-adc9-4dd1-a8e6-26c3dc5ea233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gc\n",
    "from pytabkit import TabM_D_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "    \n",
    "lr = 0.0006036532064312392\n",
    "smoothing = 4.1277849868115076e-05\n",
    "\n",
    "param_TabM_inputs = {\n",
    "    'device': 'cuda',\n",
    "    'val_metric_name': '1-auc_ovr',\n",
    "    'random_state': 100,\n",
    "    'verbosity': -1,\n",
    "    'arch_type': 'tabm-mini',\n",
    "    'tabm_k': 32,\n",
    "    'num_emb_type': 'pwl',\n",
    "    'd_embedding': 12,\n",
    "    'batch_size': 256,\n",
    "    'lr': lr,\n",
    "    'n_epochs': 10,\n",
    "    'dropout': 0.1,\n",
    "    'd_block': 512,\n",
    "    'n_blocks': 3,\n",
    "    \"compile_model\":True,\n",
    "    \"allow_amp\":True,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "    df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "    y_train = df_target.loc[train_idx]\n",
    "\n",
    "    #instantiate and fit column transformer\n",
    "    col_tran = ColumnTransformer([(\"cat\", TargetEncoder(cols=CATS, smoothing=smoothing), CATS), \n",
    "                              (\"num\", StandardScaler(), NUMS)])#, remainder=\"passthrough\")\n",
    "    df_train_cross = col_tran.fit_transform(df_train_cross, y_train)\n",
    "    df_val_cross = col_tran.transform(df_val_cross)\n",
    "\n",
    "    #instantiate and fit tabM model\n",
    "    tab_m = TabM_D_Classifier(**param_TabM_inputs)\n",
    "\n",
    "    tab_m.fit(df_train_cross, y_train)\n",
    "    oof = tab_m.predict_proba(df_val_cross)\n",
    "    input_tensor_for_meta_model[val_idx,:,0] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "    del tab_m\n",
    "    del df_train_cross, df_val_cross, y_train, oof\n",
    "    gc.collect()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d76d9c-5e50-4cca-945a-7cfd52d7a60b",
   "metadata": {},
   "source": [
    "### 2) OOF_predictions with best found hyperparameters for XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5384f521-504e-4ff7-86ae-faf7d2a06ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lr = 0.18539072905254378\n",
    "smoothing = 0.0001579817099466443\n",
    "max_depth = 4\n",
    "n_estimators=944\n",
    "\n",
    "#model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "    df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "    y_train = df_target.loc[train_idx]\n",
    "\n",
    "    preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),\n",
    "                        (\"nums\", StandardScaler(), NUMS)])\n",
    "    est = XGBClassifier(device = \"cuda\", learning_rate=lr, max_depth=max_depth, n_estimators=n_estimators)\n",
    "    model = make_pipeline(preproc, est)\n",
    "\n",
    "    model.fit(df_train_cross, y_train)\n",
    "    oof = model.predict_proba(df_val_cross)\n",
    "\n",
    "    input_tensor_for_meta_model[val_idx,:,1] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "    del model\n",
    "    del df_train_cross, df_val_cross, y_train, oof\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8e4e2-5207-4c37-9ae8-37e974e0cb15",
   "metadata": {},
   "source": [
    "### 3) OOF_predictions with best found hyperparameters for TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebcb695-7054-4d69-ab88-130bd9bf365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "\n",
    "#define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "\n",
    "lr = 0.012835801665180483\n",
    "\n",
    "weight_decay = 1.9128672660942447e-05\n",
    "\n",
    "#define model, optimizer and criterion    \n",
    "model = TabTransformer(cat_cols=CATS, num_cols=NUMS, num_hiddens=32, ffn_num_hiddens=128, num_heads=8, num_blks=6, dropout=0.2)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#define data and cross-validation splitting values\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "fold_accuracies=[]\n",
    "\n",
    "for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "\n",
    "    df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "    y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "    train_subset = MyDataset(df_train_cross, CATS, NUMS, y_train)\n",
    "    val_subset = MyDataset(df_val_cross, CATS, NUMS, y_val)\n",
    "    \n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=1024, shuffle=True, pin_memory=True, num_workers=10, \n",
    "                                                   collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_subset, batch_size=2048, shuffle=False, pin_memory=True, num_workers=10, \n",
    "                                                collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "    \n",
    "    #re-initialize weights here\n",
    "    init_inputs = (next(iter(train_dataloader)))[:2]\n",
    "    for i, v in enumerate(init_inputs):\n",
    "        v = v.to(device, non_blocking=True)\n",
    "        init_inputs[i]=v\n",
    "        \n",
    "    model.apply_init(init_inputs, model.init_weights_xavier_uniform)\n",
    "\n",
    "    num_epochs=1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs in train_dataloader:\n",
    "            #move data to GPU\n",
    "            for k,v in enumerate(inputs):\n",
    "                v = v.to(device, non_blocking=True)\n",
    "                inputs[k] = v\n",
    "            \n",
    "            outputs = model(*inputs[:2])\n",
    "            loss = criterion(outputs, inputs[2])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            del inputs\n",
    "            del loss\n",
    "            del outputs\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        i=0\n",
    "        for inputs in val_dataloader:\n",
    "\n",
    "            for k, v in enumerate(inputs):\n",
    "                v = v.to(device, non_blocking=True)\n",
    "                inputs[k] = v\n",
    "\n",
    "            oof = model(*inputs[:2]) #mlm_Y_hat and nsp_Y_hat\n",
    "\n",
    "            softmax_layer = nn.Softmax(dim=1)\n",
    "            oof = softmax_layer(oof)\n",
    "\n",
    "            input_tensor_for_meta_model[val_idx[i*2048:(i+1)*2048],:,2] = oof.to(device=\"cpu\")\n",
    "            i+=1\n",
    "\n",
    "    del df_train_cross, df_val_cross, y_train, oof\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "del model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01850a22-c107-4778-bf23-c340e051129d",
   "metadata": {},
   "source": [
    "### 4) OOF_predictions with best found hyperparameters for LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a97f1b6-4497-443e-a7ae-8bfc0955545d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "\n",
    "\n",
    "\n",
    "lr = 0.019033709394977512\n",
    "smoothing = 0.011297091655658249\n",
    "n_estimators= 867\n",
    "num_leaves = 24\n",
    "\n",
    "params = {\n",
    "\"boosting_type\": \"gbdt\",\n",
    "\"objective\": \"binary\",\n",
    "\"metric\": {\"l2\", \"l1\"},\n",
    "\"num_leaves\": num_leaves,\n",
    "\"learning_rate\": lr,\n",
    "\"feature_fraction\": 0.9,\n",
    "\"bagging_fraction\": 0.8,\n",
    "\"bagging_freq\": 5,\n",
    "\"verbose\": -1,\n",
    "\"device_type\":\"cuda\",\n",
    "\"n_estimators\":n_estimators\n",
    "}\n",
    "\n",
    "#model = make_pipeline(col_tran, tab_m)\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "    df_train_cross, df_val_cross = df_train.loc[train_idx, CATS+NUMS], df_train.loc[val_idx, CATS+NUMS]\n",
    "    y_train, y_val = df_target.loc[train_idx], df_target.loc[val_idx]\n",
    "\n",
    "    preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),(\"nums\", StandardScaler(), NUMS)])\n",
    "\n",
    "    df_train_cross = preproc.fit_transform(df_train_cross, y_train)\n",
    "    df_val_cross = preproc.transform(df_val_cross)\n",
    "\n",
    "\n",
    "    lgb_train = lgb.Dataset(df_train_cross, y_train, categorical_feature=None)\n",
    "    \n",
    "    lgb_eval = lgb.Dataset(df_val_cross, y_val, reference=lgb_train, categorical_feature=None)\n",
    "\n",
    "    #gbm = lgb.LGBMClassifier(**params)\n",
    "\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round=20, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(stopping_rounds=5)])\n",
    "\n",
    "    #gbm.fit(df_train_cross, y_train)\n",
    "    oof = gbm.predict(df_val_cross, type=\"response\")\n",
    "\n",
    "    prob_pos_class = torch.tensor(oof).to(device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "    prob_neg_class = 1-prob_pos_class\n",
    "\n",
    "    X = torch.stack([prob_neg_class, prob_pos_class], dim=1)\n",
    "\n",
    "    input_tensor_for_meta_model[val_idx,:,3] = X\n",
    "\n",
    "    del gbm\n",
    "    del df_train_cross, df_val_cross, y_train, y_val, X\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fa48be-86c8-4ead-bc64-f4b4ccd22c3d",
   "metadata": {},
   "source": [
    "### 5) OOF_predictions with best found hyperparameters for RealMLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0be80c-d0c3-41ad-b84c-f1ad04c86a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_tensor_from_realmlp = torch.zeros(len(df_train), 2, 1)\n",
    "\n",
    "from pytabkit import RealMLP_TD_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")    #requires tensor cores. Trades precision for performance\n",
    "\n",
    "df_train_realmlp = df_train.copy()\n",
    "\n",
    "for col in CATS:\n",
    "    df_train_realmlp[col] = df_train_realmlp[col].cat.codes\n",
    "\n",
    "    \n",
    "lr = 0.8301728407699202\n",
    "\n",
    "\n",
    "kf = StratifiedKFold(n_splits=3, shuffle=True)\n",
    "\n",
    "\n",
    "for train_idx, val_idx in kf.split(np.zeros(len(df_train)), df_target):\n",
    "    df_train_cross, df_val_cross = df_train_realmlp.loc[train_idx, CATS+NUMS], df_train_realmlp.loc[val_idx, CATS+NUMS]\n",
    "    y_train = df_target.loc[train_idx] \n",
    "\n",
    "    model = RealMLP_TD_Classifier(device='cuda:0', random_state=0, n_cv=1, n_refit=0,\n",
    "                              n_epochs=10, batch_size=1024, hidden_sizes=[256] * 3,\n",
    "                              val_metric_name='cross_entropy',\n",
    "                              use_ls=False,\n",
    "                              lr=lr, \n",
    "                              verbosity=-1, val_fraction=0)\n",
    "    \n",
    "    model.fit(df_train_cross, y_train, cat_col_names=CATS)\n",
    "    oof = model.predict_proba(df_val_cross)\n",
    "\n",
    "    input_tensor_from_realmlp[val_idx,:,0] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "    del model, oof\n",
    "    del df_train_cross, df_val_cross, y_train\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160fe4c-3c07-4ebd-bf52-887e79e2fbc4",
   "metadata": {},
   "source": [
    "# Save input_tensor_for_meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1816a-2fb0-4d0f-9321-9749c79eded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "tensor_to_save = {\"my_first_tensor\":input_tensor_for_meta_model}\n",
    "\n",
    "save_file(tensor_to_save, \"input_to_meta_model.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e84fa5-cc5b-458a-9b24-ffd02a31a870",
   "metadata": {},
   "source": [
    "## Train meta_model: We choose CatBoost meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34326ad4-ef13-4d4c-9b09-d53777b908c8",
   "metadata": {},
   "source": [
    "### 0.0) load input_to_meta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c738e41-64ba-4e23-ab67-e819cb5b58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "loaded_tensor = load_file(\"input_to_meta_model.safetensors\")\n",
    "\n",
    "input_tensor_for_meta_model = loaded_tensor[\"my_first_tensor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e83ea-8000-41cd-a05e-65a25ae0bfcc",
   "metadata": {},
   "source": [
    "### 0) Convert input torch tensor to numpy tensor and reshape from len(df_train),2,4 to len(df_train),8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7f717f-b78f-4c22-a9f5-d455af658a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_input = input_tensor_for_meta_model.reshape(-1,10).numpy()\n",
    "meta_target = df_target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4bdc56-b77d-4fea-82de-71a4b089758d",
   "metadata": {},
   "source": [
    "### 1) Train CatBoost meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253b0f0-4390-4d79-bce2-5db0ab15ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Initialize the CatBoost classifier\n",
    "catboost_model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, verbose=0, task_type='GPU')\n",
    "\n",
    "# Train the CatBoost model\n",
    "catboost_model.fit(meta_input, meta_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f35943-0420-488e-91b2-e61d9c76f805",
   "metadata": {},
   "source": [
    "### 1.1) Let's save the catboost (meta) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809d751d-042c-4467-88fa-4632c62096f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model.save_model(\"dirty_data_ensemble.cbm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a3c71-2805-477b-bc38-54ebd0432a36",
   "metadata": {},
   "source": [
    "## Train each base model on the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf03c5-65f8-4649-85d3-c3706ecdf7c2",
   "metadata": {},
   "source": [
    "### 1) Train TabM_D_Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b03ed-2517-4a50-822e-ff1f00f56d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gc\n",
    "from pytabkit import TabM_D_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "\n",
    "#Use the best hyperparameters again\n",
    "    \n",
    "lr = 0.0006036532064312392\n",
    "smoothing = 4.1277849868115076e-05\n",
    "\n",
    "param_TabM_inputs = {\n",
    "    'device': 'cuda:0',\n",
    "    'val_metric_name': '1-auc_ovr',\n",
    "    'random_state': 100,\n",
    "    'verbosity': 2,\n",
    "    'arch_type': 'tabm-mini',\n",
    "    'tabm_k': 32,\n",
    "    'num_emb_type': 'pwl',\n",
    "    'd_embedding': 12,\n",
    "    'batch_size': 512,\n",
    "    'lr': lr,\n",
    "    'n_epochs': 10,\n",
    "    'dropout': 0.1,\n",
    "    'd_block': 512,  ##smaller for shorter training\n",
    "    'n_blocks': 3,   ##smaller for shorted training\n",
    "    \"compile_model\":True,\n",
    "    \"allow_amp\":True,\n",
    "    \"val_fraction\":0.4\n",
    "}\n",
    "\n",
    "col_tran = ColumnTransformer([(\"cat\", TargetEncoder(cols=CATS, smoothing=smoothing), CATS), \n",
    "                          (\"num\", StandardScaler(), NUMS)])#, remainder=\"passthrough\")\n",
    "df_train_tabm = col_tran.fit_transform(df_train, df_target)\n",
    "\n",
    "\n",
    "#instantiate and fit tabM model\n",
    "tab_m = TabM_D_Classifier(**param_TabM_inputs)\n",
    "\n",
    "tab_m.fit(df_train_tabm, df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82554c2-1047-4cc8-9c1e-489f130e4182",
   "metadata": {},
   "source": [
    "### 1.1) Save the tab_m model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef4342-6c72-4135-8e64-c54fca9bd5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assume 'model' is your trained machine learning model\n",
    "filename = 'my_tabm_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(tab_m, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cf5c35-ddb1-46b2-ad33-3de9380632fb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### 2)Train XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ae403-fa6d-4aab-a13f-e1e4396ecfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lr = 0.18539072905254378\n",
    "smoothing = 0.0001579817099466443\n",
    "max_depth = 4\n",
    "n_estimators=944\n",
    "\n",
    "preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),\n",
    "                    (\"nums\", StandardScaler(), NUMS)])\n",
    "est = XGBClassifier(device = \"cuda\", learning_rate=lr, max_depth=max_depth, n_estimators=n_estimators)\n",
    "xgb_model = make_pipeline(preproc, est)\n",
    "\n",
    "xgb_model.fit(df_train, df_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c112fcd6-b784-4f10-87a0-d4a34b23a8e2",
   "metadata": {},
   "source": [
    "### 2.1) Save XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ac5d0-645d-4f29-beda-27755c471af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assume 'model' is your trained machine learning model\n",
    "filename = 'my_xgb_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(xgb_model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ba61e9-44ce-4f51-89ba-7b4af6d7d4d8",
   "metadata": {},
   "source": [
    "### 3) Train TabTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65fb75d-8a7f-4940-b7cd-6b6226c57e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "\n",
    "\n",
    "#define hyperparameter search space. Keep it simple and just search \"lr\" and \"weight_decay\"\n",
    "\n",
    "lr = 0.012835801665180483\n",
    "\n",
    "weight_decay = 1.9128672660942447e-05\n",
    "\n",
    "#define model, optimizer and criterion    \n",
    "model = TabTransformer(cat_cols=CATS, num_cols=NUMS, num_hiddens=32, ffn_num_hiddens=128, num_heads=8, num_blks=6, dropout=0.2)\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "train_subset = MyDataset(df_train, CATS, NUMS, df_target)\n",
    "\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_subset, batch_size=512, shuffle=True, pin_memory=True, num_workers=10, \n",
    "                                               collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "\n",
    "#re-initialize weights here\n",
    "init_inputs = (next(iter(train_dataloader)))[:2]\n",
    "for i, v in enumerate(init_inputs):\n",
    "    v = v.to(device, non_blocking=True)\n",
    "    init_inputs[i]=v\n",
    "    \n",
    "model.apply_init(init_inputs, model.init_weights_xavier_uniform)\n",
    "\n",
    "num_epochs=1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs in train_dataloader:\n",
    "        #move data to GPU\n",
    "        for k,v in enumerate(inputs):\n",
    "            v = v.to(device, non_blocking=True)\n",
    "            inputs[k] = v\n",
    "        \n",
    "        outputs = model(*inputs[:2])\n",
    "        loss = criterion(outputs, inputs[2])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        del inputs\n",
    "        del loss\n",
    "        del outputs\n",
    "\n",
    "#################################################################################\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb4bc2f-97a0-4441-9520-80ef4766b427",
   "metadata": {},
   "source": [
    "### 3.1) Save TabTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1636977-9957-4430-b920-3fa888b5aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "\n",
    "save_file(state_dict, \"tab_transformer_model.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ecfa2e-8fca-43a5-9243-260d96d58302",
   "metadata": {},
   "source": [
    "### 4) Train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9566cc-4da5-48a6-a819-56b223eb2104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "lr = 0.019033709394977512\n",
    "smoothing = 0.011297091655658249\n",
    "n_estimators= 867\n",
    "num_leaves = 24\n",
    "\n",
    "params = {\n",
    "\"boosting_type\": \"gbdt\",\n",
    "\"objective\": \"binary\",\n",
    "\"metric\": {\"l2\", \"l1\"},\n",
    "\"num_leaves\": num_leaves,\n",
    "\"learning_rate\": lr,\n",
    "\"feature_fraction\": 0.9,\n",
    "\"bagging_fraction\": 0.8,\n",
    "\"bagging_freq\": 5,\n",
    "\"verbose\": -1,\n",
    "\"device_type\":\"cuda\",\n",
    "\"n_estimators\":n_estimators\n",
    "}\n",
    "\n",
    "preproc = ColumnTransformer([(\"cats\", TargetEncoder(cols=CATS, smoothing = smoothing), CATS),\n",
    "                    (\"nums\", StandardScaler(), NUMS)])\n",
    "\n",
    "df_train_cross = preproc.fit_transform(df_train, df_target)\n",
    "\n",
    "lgb_train = lgb.Dataset(df_train_cross, df_target, categorical_feature=None)\n",
    "\n",
    "gbm = lgb.train(params, lgb_train, num_boost_round=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b228e5-a1c2-4844-b079-faebb0d8a69e",
   "metadata": {},
   "source": [
    "### 4.1) Save LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6c9c1-1fd3-4bbb-9324-4792dc35f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.save_model(\"lightgbm.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fcd4f0-39d2-43a5-aad7-ebc1870b5d7d",
   "metadata": {},
   "source": [
    "### 5) Train RealMLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc6087-5901-4c55-b06a-dcf99936e762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytabkit import RealMLP_TD_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")    #requires tensor cores. Trades precision for performance\n",
    "\n",
    "df_train_realmlp = df_train.copy()\n",
    "\n",
    "for col in CATS:\n",
    "    df_train_realmlp[col] = df_train_realmlp[col].cat.codes\n",
    "\n",
    "lr = 0.8301728407699202\n",
    "\n",
    "model = RealMLP_TD_Classifier(device='cuda:0', random_state=0, n_cv=1, n_refit=0,\n",
    "                          n_epochs=10, batch_size=1024, hidden_sizes=[256] * 3,\n",
    "                          val_metric_name='cross_entropy',\n",
    "                          use_ls=False,\n",
    "                          lr=lr, \n",
    "                          verbosity=-1, val_fraction=0)\n",
    "\n",
    "model.fit(df_train_realmlp, df_target, cat_col_names=CATS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d53da-e6e8-4e61-a37c-bfd6cdb93949",
   "metadata": {},
   "source": [
    "### 5.1) Save RealMLP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6226bb-c9ad-4f9a-b6a0-d01c8c7a6194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assume 'model' is your trained machine learning model\n",
    "filename = 'my_realmlp_model.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f82268-09eb-4e43-a365-91365d4181c4",
   "metadata": {},
   "source": [
    "## 4) Create stacked (generalization model) of previous base learners and meta-model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ac1936-e3ad-4dec-9111-ef8eda85fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtest = \"/home/ildar/.cache/kagglehub/datasets/amruthayenikonda/dirty-dataset-to-practice-data-cleaning/versions/1/test.csv\"\n",
    "\n",
    "test = pd.read_csv(pathtest, index_col=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33313386-12dd-4c22-a544-408acbde3372",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf97cdf-ea84-4d3b-92fc-05cae558f088",
   "metadata": {},
   "source": [
    "### 4.1) Perform the same preprocessing operations on test as was done for df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b822e8b-90c9-4bef-a354-a939e4f3ac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cyclical features\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "df_test = test.copy()\n",
    "\n",
    "df_test['_pdays_sin'] = np.sin(2*np.pi * df_test['pdays'] / 365).astype('float32')\n",
    "df_test['_pdays_cos'] = np.cos(2*np.pi*df_test['pdays']/365).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13395ebd-2882-4f37-9fac-6c7789725a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert balance and duration to float32\n",
    "\n",
    "df_test[\"balance\"] = df_test[\"balance\"].astype(\"float32\")\n",
    "df_test[\"duration\"] = df_test[\"duration\"].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d464a4-c49b-4cc3-a48b-3bd02360ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_values = {'may': 5,\n",
    " 'aug': 8,\n",
    " 'jul': 7,\n",
    " 'jun': 6,\n",
    " 'nov': 11,\n",
    " 'apr': 4,\n",
    " 'feb': 2,\n",
    " 'jan': 1,\n",
    " 'oct': 10,\n",
    " 'sep': 9,\n",
    " 'mar': 3,\n",
    " 'dec': 12}\n",
    "\n",
    "df_test[\"data\"] = (df_test[\"month\"].replace(replacement_values)-1)*12+df_test[\"day\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef937526-18f9-4a35-af0b-e48bad2b938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"_date_cos\"] = np.cos(2*np.pi*df_test[\"data\"]/365).astype(\"float32\")\n",
    "df_test[\"_date_sin\"] = np.sin(2*np.pi*df_test[\"data\"]/365).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d19e16-2864-44ef-84e5-0207909bacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"was_not_contacted\"] = (df_test[\"pdays\"]==-1).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78d42e-ce1d-488d-b004-affb2a3c876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert duration and balance by logarithms\n",
    "\n",
    "df_test[\"balance_log\"] = np.sign(df_test[\"balance\"])*np.log1p(np.abs(df_test[\"balance\"]))\n",
    "df_test[\"duration_log\"] = np.log1p(df_test[\"duration\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f50c3a-71e6-4c97-b03c-41e50402cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert int64 and object dtypes to category\n",
    "cols = df_test.select_dtypes([\"int64\", \"object\"]).columns.to_list()\n",
    "\n",
    "for col in cols:\n",
    "    df_test[col] = df_test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2946ea-d4d5-4e1f-8a5e-64747e14aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "CATS = df_test.select_dtypes(\"category\").columns.to_list()\n",
    "NUMS = df_test.select_dtypes(\"float32\").columns.to_list()\n",
    "\n",
    "for col in combinations(CATS,2):\n",
    "    name = \"_\".join(col)\n",
    "    df_test[name] = (df_test[col[0]].astype(\"str\")+\"_\"+df_test[col[1]].astype(\"str\")).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a953e5-4633-42c8-83d6-0af84316decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in combinations(NUMS, 2):\n",
    "    name = \"x\".join(col)\n",
    "    df_test[name] = df_test[col[0]]*df_test[col[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e94b9f-896d-4dd8-8990-dc88c4adb323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44f6948-63ac-4d92-bfad-f4867e90925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATS = df_test.select_dtypes(\"category\").columns.to_list()\n",
    "NUMS = df_test.select_dtypes(\"number\").columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371fcf6-d338-422b-a8c1-a9ee8e45db8b",
   "metadata": {},
   "source": [
    "### 4.2) Build the input for meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0fdc8d-d3d6-4537-a347-4e958b4bbdb3",
   "metadata": {},
   "source": [
    "### 4.2.0) Set-up input for the meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb23b1-9e9e-4d02-85fb-a46972f340dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_for_meta_model_preds = torch.zeros(len(df_test), 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0ddc7-e2a3-4308-8306-985bc45ac90f",
   "metadata": {},
   "source": [
    "### 4.2.1.0) Load TabM_D_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd019db-6294-4530-98a8-454902c65e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Assume 'model' is your trained machine learning model\n",
    "filename = 'my_tabm_model.pkl'\n",
    "\n",
    "with open(filename, 'rb') as file:\n",
    "    tabm_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314832b-3dfb-42ec-b4a3-8161964f3e06",
   "metadata": {},
   "source": [
    "### 4.2.1.1) Outputs of TabM_D_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fda92f-9c43-48d0-91d3-fde72febe36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gc\n",
    "from pytabkit import TabM_D_Classifier\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smoothing = 4.1277849868115076e-05\n",
    "\n",
    "col_tran = ColumnTransformer([(\"cat\", TargetEncoder(cols=CATS, smoothing=smoothing), CATS), \n",
    "                          (\"num\", StandardScaler(), NUMS)])#, remainder=\"passthrough\")\n",
    "col_tran.fit(df_train, y=df_target)\n",
    "\n",
    "\n",
    "df_test_tabm = col_tran.transform(df_test)\n",
    "\n",
    "oof = tabm_model.predict_proba(df_test_tabm)\n",
    "input_tensor_for_meta_model_preds[:,:,0] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "del tabm_model\n",
    "del oof\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef8109-c080-4a5b-9186-830373cd886b",
   "metadata": {},
   "source": [
    "### 4.2.2.0) Load XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e7135-719f-456e-9e2a-792354e2a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'my_xgb_model.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    xgb_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bf1cd9-6e46-442e-8366-405d9c77e8a5",
   "metadata": {},
   "source": [
    "### 4.2.2.1) Outputs of XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e74088c-e965-476d-8ee5-71be18395587",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof = xgb_model.predict_proba(df_test)\n",
    "input_tensor_for_meta_model_preds[:,:,1] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "del xgb_model\n",
    "del oof\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8554b62-caab-467c-a895-c743d10f82f7",
   "metadata": {},
   "source": [
    "### 4.2.3.0) Load TabTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5081f-eedf-4e75-bd22-ade5a5a14843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "tabt_model = TabTransformer(cat_cols=CATS, num_cols=NUMS, num_hiddens=32, ffn_num_hiddens=128, num_heads=8, num_blks=6, dropout=0.2)\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "tabt_model.to(device)\n",
    "\n",
    "state_dict = load_file(\"tab_transformer_model.safetensors\")\n",
    "\n",
    "# Load the state_dict into the model\n",
    "tabt_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc474660-7858-4f69-b5da-db2e1d80942f",
   "metadata": {},
   "source": [
    "### 4.2.3.1) Outputs of TabTransformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349a812-78a7-4b52-a860-01143c2bc8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = MyDataset(df_test, CATS, NUMS, df_target[:250000])\n",
    "\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_subset, batch_size=2048, shuffle=False, pin_memory=True, num_workers=10, \n",
    "                                               collate_fn = collate_fn, prefetch_factor=2, multiprocessing_context='fork')\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "\n",
    "tabt_model.eval()\n",
    "with torch.no_grad():\n",
    "    i=0\n",
    "    for inputs in test_dataloader:\n",
    "\n",
    "        for k, v in enumerate(inputs):\n",
    "            v = v.to(device, non_blocking=True)\n",
    "            inputs[k] = v\n",
    "\n",
    "        oof = tabt_model(*inputs[:2]) #mlm_Y_hat and nsp_Y_hat\n",
    "\n",
    "        softmax_layer = nn.Softmax(dim=1)\n",
    "        oof = softmax_layer(oof)\n",
    "\n",
    "        input_tensor_for_meta_model_preds[i*2048:(i+1)*2048,:,2] = oof.to(device=\"cpu\")\n",
    "        i+=1\n",
    "\n",
    "        del oof\n",
    "del tabt_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec073d-fceb-49c6-b440-166466a01a8d",
   "metadata": {},
   "source": [
    "### 4.2.4.0) Load LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01544197-3d65-4faf-b605-93e25a57ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "bst_loaded = lgb.Booster(model_file=\"lightgbm.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19317d21-0b95-4fce-bcad-2ab39fb956a0",
   "metadata": {},
   "source": [
    "### 4.2.4.1) Outputs of LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29209796-472b-4394-b2b9-0e2bd0b5b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_tran = ColumnTransformer([(\"cat\", TargetEncoder(cols=CATS, smoothing=smoothing), CATS), \n",
    "                          (\"num\", StandardScaler(), NUMS)])#, remainder=\"passthrough\")\n",
    "col_tran.fit(df_train, y=df_target)\n",
    "\n",
    "\n",
    "df_test_lightgbm = col_tran.transform(df_test)\n",
    "\n",
    "\n",
    "\n",
    "oof = bst_loaded.predict(df_test_lightgbm, type=\"response\")\n",
    "\n",
    "prob_pos_class = torch.tensor(oof).to(device=\"cpu\", dtype=torch.float32)\n",
    "\n",
    "prob_neg_class = 1-prob_pos_class\n",
    "\n",
    "X = torch.stack([prob_neg_class, prob_pos_class], dim=1)\n",
    "\n",
    "input_tensor_for_meta_model_preds[:,:,3] = X\n",
    "\n",
    "del bst_loaded\n",
    "del oof, X\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0578b5-eec6-4430-bf76-89f0af4857f3",
   "metadata": {},
   "source": [
    "### 4.2.5.0) Load RealMLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49eb5c-51f6-42df-a79c-97a1133730ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'my_realmlp_model.pkl'\n",
    "with open(filename, 'rb') as file:\n",
    "    realmlp_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47a7a62-c78b-409f-beec-c5a8c32c2162",
   "metadata": {},
   "source": [
    "### 4.2.5.1) Outputs of RealMLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd4b6a9-0bc5-4b39-bfff-1ddde6ee935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_from_realmlp_preds = torch.zeros(len(df_test), 2, 1)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")    #requires tensor cores. Trades precision for performance\n",
    "\n",
    "df_test_realmlp = df_test.copy()\n",
    "\n",
    "for col in CATS:\n",
    "    df_test_realmlp[col] = df_test_realmlp[col].cat.codes\n",
    "\n",
    "oof = realmlp_model.predict_proba(df_test_realmlp)\n",
    "input_tensor_from_realmlp_preds[:,:,0] = torch.tensor(oof).to(device=\"cpu\")\n",
    "\n",
    "del realmlp_model\n",
    "del oof\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f3558d-9b07-462d-8696-f2e8c9c54da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "loaded_tensor = load_file(\"input_to_meta_model_preds.safetensors\")\n",
    "\n",
    "input_to_meta_model_preds = loaded_tensor[\"my_first_tensor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f9d99-1a12-4f7d-8cc2-e4e3a9cad0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_meta_model_preds = torch.cat([input_to_meta_model_preds, input_tensor_from_realmlp_preds], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4837b-dabd-44a1-89cb-05b494f503e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_for_meta_model_preds = input_to_meta_model_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8252b3a-10c3-4828-8043-69387a5276b0",
   "metadata": {},
   "source": [
    "### 4.3) Save input for meta-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f91760a-99a8-4e81-b2b9-b72d37fa4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors.torch import save_file\n",
    "\n",
    "tensor_to_save = {\"my_first_tensor\":input_tensor_for_meta_model_preds}\n",
    "\n",
    "save_file(tensor_to_save, \"input_to_meta_model_preds.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c800e-3879-4d8f-bd05-7156392e5a45",
   "metadata": {},
   "source": [
    "### 4.3) Inference of meta-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb0d7f-87bd-4b2e-ba70-72a74a74bb4f",
   "metadata": {},
   "source": [
    "### 4.3.1) Prepare input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ec8a5-fa52-44a8-9223-6932a3ea6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_input_preds = input_tensor_for_meta_model_preds.reshape(-1,10).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7817fd92-d431-45e9-af84-243e15048489",
   "metadata": {},
   "source": [
    "### 4.3.2) load meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8536f-3670-4fdc-9c6d-bb91cb0913de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembler = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, random_state=42, verbose=0, task_type='GPU')\n",
    "ensembler.load_model(\"dirty_data_ensemble.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2aa15d-a401-48b3-b2dd-63e8aad57da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ensembler.predict_proba(meta_input_preds)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf69ceeb-38d6-4ed2-91c4-20acf9312552",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c51ef8e-52aa-456a-b199-4b88851f83d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission[[\"id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2295a-70a9-46fa-b497-43023939f008",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"Y\"] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bcba5e-d22e-4018-bb05-b36301381a5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv(\"/home/ildar/out.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587ffa4-bf73-4217-be8f-1b0b12593fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
